**Paper title:**

Shortcut Mining: Exploiting Cross-layer Shortcut Reuse in DCNN Accelerators

**Publication:**

HPCA’19

**Problem to solve:**

Off-chip memory traffic has been a major performance bottleneck in deep learning
accelerators. While reusing on-chip data is a promising way to reduce off-chip
traffic, the opportunity on reusing shortcut connection data in deep networks
(e.g., residual networks) have been largely neglected. Those shortcut data
accounts for nearly 40% of the total feature map data:

-   The lack of flexibility in existing buffer architecture prevents on-chip
    buffers from being allocated and utilized at the needed granularity

-   Reusing shortcut data requires the data to be somehow preserved across
    multiple layers, which is difficulty without incurring extra buffer overhead

-   It is challenging to provide a unified solution that works for different
    networks, as the building block could contain an arbitrary number of layers

**Major contribution:**

This paper proposes Shortcut Mining (SCM), a novel approach that “mines” the
largely unexplored opportunity of reusing shortcut and feature map data to
reduce off-chip traffic. To achieve this objective, several major challenges
must be addressed: (1) the lack of flexibility in existing buffer architecture
prevents on-chip buffers from being allocated and utilized at the needed
granularity; (2) reusing shortcut data requires the data to be somehow preserved
across multiple layers, which is difficulty without incurring extra buffer
overhead; and (3) it is challenging to provide a unified solution that works for
different networks, as the building block could contain an arbitrary number of
layers. Thus, the shortcut data needs to be preserved for any number of layers.

In this paper, they address the first challenge by introducing the use of
logical buffers that are formed dynamically from a pool of physical buffer
banks. This allows buffer operations to be done at individual bank level,
without physically moving the data. Enabled by this, they develop three
procedures, namely Prolog, Kernel and Epilog, that work collaboratively to allow
shortcut data to be reused across any number of layers in the building block of
DCNNs. On the high level, the basic idea is for the Prolog procedure to save the
shortcut data of a CNN layer in a certain region of the on-chip buffer called
the invariant buffer space. Later after several layers, the Epilog procedure can
restore the shortcut data from the invariant buffer space for the shortcut to be
reused. In the meantime, the Kernel procedure is executed multiple times, one
for each layer in a building block. At each layer, the Kernel reuses the
non-shortcut feature maps of the previous layer as the input of the current
layer and, at the same time, keep the invariant buffer space untouched,
regardless of how many times the Kernel is executed. The collective result of
the three procedures is that shortcut feature maps are preserved across layers
while non-shortcut feature maps are reused immediately after each layer.

**Lessons learnt:**

This paper solves the off-chip memory traffic by exploiting shortcut connection
data. Those shortcut data accounts for nearly 40% of the total feature map data.
They introduce the abstraction of logical buffers to address the lack of
flexibility in existing buffer architecture, and then propose a sequence of
procedures which, collectively, can effectively reuse both shortcut and
non-shortcut feature maps. The proposed procedures are also able to reuse
shortcut data across any number of intermediate layers without using additional
buffer resources.
