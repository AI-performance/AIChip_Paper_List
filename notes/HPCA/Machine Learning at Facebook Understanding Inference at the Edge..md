**Paper Titleï¼š**

Machine Learning at Facebook Understanding Inference at the Edge

**Publication:**

HPCA 2019

**Problem to Solve:**

While machine learning models are currently trained on customized datacenter
infrastructure, Facebook is working to bring machine learning inference to the
edge. By doing so, user experience is improved with reduced latency (inference
time) and becomes less dependent on network connectivity. Furthermore, this also
enables many more applications of deep learning with important features only
made available at the edge.

**Major Contributions:**

1.  Take a vertical integration approach to enable inference for systems. By
    doing so, performance acceleration with co-processors is far more realistic,
    leading to faster inference time, lower power consumption, and more
    importantly reduced performance variability.

2.  Demonstrate the possibilities of deep learning inference at the edge, and
    pinpoint the importance of in-field studies in order to capture the
    performance variability effect in the production environment.
