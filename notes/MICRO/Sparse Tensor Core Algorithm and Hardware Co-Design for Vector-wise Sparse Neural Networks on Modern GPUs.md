**Paper title:**

Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse
Neural Networks on Modern GPUs

**Publication:**

MICROâ€™19

**Problem to solve:**

To improve the computation efficiency of neural networks, many pruning
techniques have been proposed to reduce the amount of multiply-accumulate (MAC)
operations, which results in high sparsity in the networks. However, the sparse
neural networks often run slower than their dense counterparts on modern GPUs
due to their poor device utilization rate. Furthermore, Tensor Core focuses only
on the acceleration of dense matrix multiplication, and sparse GEMM cannot take
advantage of Tensor Core, so the performance of sparse neural networks lags
behind more significantly.

**Major contribution:**

To tackle above problem, the paper proposes *VectorSparse*, a SIMD-friendly
sparsifying method. The contributions of this paper are as follows:

1.  This is the first work to exploit the efficiency of sparse neural networks
    on Tensor Core.

2.  Through a comprehensive performance analysis to demonstrate the inefficiency
    of GPU when running the sparse neural networks

3.  The sparse weight matrices generated by VectorSparse exhibit a better
    workload balance and higher parallelism than the top-K pruned weight
    matrices. VectorSparse as a novel sparsifying algorithm can achieve 63%
    performance improvement with negligible accuracy drop.

4.  Further extend the instruction sets of the Volta GPU to support the operand
    indexing in the register file.

The experimental results show that the pruning algorithm can achieve 63%
performance gain with model accuracy sustained. Furthermore, the hardware
optimization gives an additional 58% performance gain with negligible area
overhead.

**Lessons learnt:**

The combination of algorithms and hardware design often leads to better
performance.
