**Paper title:**

Bit-Tactical: A Software/Hardware Approach to Exploiting Value and Bit Sparsity
in Neural Networks

**Publication:**

ASPLOS '19

**Problem to solve:**

Weight and activation sparsity can be leveraged in hardware to boost the
performance and energy efficiency of Deep Neural Networks during inference.
Fully capitalizing on sparsity requires re-scheduling and mapping the execution
stream to deliver non-zero weight/activation pairs to multiplier units for
maximal utilization and reuse. However, permitting arbitrary value re-scheduling
in memory space and in time places a considerable burden on hardware to perform
dynamic at-runtime routing and matching of values, and incurs significant energy
inefficiencies

In summary, fully exploiting activation and weight sparsity translates into
motion of values in the memory space, in time (execution schedule), and
spatially over the available multipliers and accumulators. Existing sparse
network accelerators allow nearly unrestricted motion of values in memory space
and in time, placing the burden on hardware to take advantage of sparsity at
runtime. There is untapped potential to better balance the responsibility of
exploiting sparsity between hardware and software

**Major contribution:**

Rather than skipping zero weights and activations, they propose to skip zero
weights and the ineffectual bits of the activations (bit-level sparsity). This
approach 1) eliminates the need to handle the dynamically sparse activation
stream, and 2) exposes more ineffectual work.

Present a novel class of front-end hardware designs that allow zero weight
skipping where all weight promotions are performed by a software scheduler and
all corresponding activation promotions are pre-planned by the scheduler and
performed at runtime via a lightweight, sparse interconnect.

Formulate a novel weight scheduling algorithm codesigned with the aforementioned
hardware connectivity that extracts most of the performance possible from a
sparse neural network given the hardware constraints.

By separating the front-end weight scheduling from the back-end
multiply-accumulate (MAC) operations, this paper explores the
energy/area/performance trade-off for two designs (TCLe and TCLp) that offer
different performance, area, and energy trade-offs. The benefits of the front-
and back-end are shown to be nearly multiplicative.

TCLe, provides an average speedup over a dense baseline accelerator and SCNN of
11.3× and 5.14× respectively, whilst being 2.13× and 2.23× more energy
efficient. TCLe improves performance by 7.2× even with 8b range-oblivious
quantization. TCLp, provides speedups of 6.7× and 3.05× respectively, while
incurring just a 10% area overhead and being slightly more energy efficiency.
TCLe and TCLp achieve an average effective 17.6 and 10.4 TOPS at 1.04 and 1.11
TOPS/W, respectively estimated post layout with a 65nm technology node.
